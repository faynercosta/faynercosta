{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIzjb0hF0AIgoqIhiUEwzB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faynercosta/faynercosta/blob/main/twitter_video_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube_transcript_api\n",
        "!pip install openai\n",
        "!pip install moviepy\n",
        "!pip install pytube\n",
        "!pip install bs4\n",
        "!pip install tweepy\n",
        "\n",
        "\n",
        "from oauth2client.tools import argparser\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import openai\n",
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
        "from pytube import YouTube\n",
        "import json\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials, db\n",
        "import datetime\n",
        "from datetime import datetime, timedelta, timezone\n",
        "import pytz\n",
        "from youtube_transcript_api import TranscriptsDisabled\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "import time\n",
        "from requests_oauthlib import OAuth1Session, OAuth1\n",
        "import tweepy\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2RGKbFMsRCj",
        "outputId": "cead563b-f2f7-4a02-de28-b074d71b99f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: youtube_transcript_api in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube_transcript_api) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (3.4)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.27.1)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.22.4)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.25.1)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.8)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n",
            "Requirement already satisfied: pytube in /usr/local/lib/python3.10/dist-packages (15.0.0)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.4.1)\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.10/dist-packages (4.13.0)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.10/dist-packages (from tweepy) (2.27.1)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_updated_data():\n",
        "  # Fetch the service account key JSON file contents\n",
        "  # Step 1: Initialize Firebase\n",
        "  cred = credentials.Certificate({\n",
        "    \"type\": \"service_account\",\n",
        "    \"project_id\": \"twitter-bot-681fc\",\n",
        "    \"private_key_id\": \"eab07d2a90ebedac18abc0ebe6efbb3f4e632afe\",\n",
        "    \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDMlY8PJxRZJ+x2\\n26dmrTDYkA9F2C5kM76VrKMsb805racNfd56vP21q3UyrxXBkgr66deftJM1GyGa\\nzsd8Khu5GdDASBSeJG1ELp+AKRU4r9K2IkbU2VAxvZz2rzJJ1wiY7c+7iI9OsS8H\\ngt9NvTfn09p74XHOfOYieiRCPds7/Wl0/f2QRvynUahUaSeUSClVKZHPRwgHjZm9\\ng/4FMn8XeHFQT4hkkGT8sVIPmhLw+6+snsQCL+M7yJ7wNAAYKXFZrVH90rB5g5LC\\n4rRFw5Hik0k9Jj1NjQoF3SqlGMhWW87JtV4CX5iUS53sY8pnRs45TnBHXxbVDOu/\\n+bFqQNBRAgMBAAECggEABFOdZOqkqYYIoDLhBXDgOZRCXm3F/WSCwVOXV9c9ddzQ\\nTZ6918arbu4jWmENirx18vyDDOOawWLDgSyiV2PkljOPkKF1niqTDQAXa3Ry+HHv\\neN6ctN+rrIGgEi9+VsVvbxyIaiTnMvjYT8ZaxqAb0JFQa5JG/+DwVQZUQfpg6pHV\\nt6HNkk5ZxSxidET+OJIA29IofGqCuu2bUZ1k4MJ0R8zGKLmeLyKDo0RQ3eUVzStE\\nOpd/pVSNvo54Eavrm+GL5wzZcXbbC+uthxDLto2slYR5G5VFmmSActu7dLb+EXHK\\nWI3IpCRFfbGANfhD7HgYdgj7YqlTypyFtjtwSAO1YQKBgQD0hl/hv7kw3xmlC6yW\\nOiIw5C28miWWOdOn3zWnx8/R/Qk+4tvI3BNaY0Y3eXwFChynv0bdolGcOYLy/krQ\\nT909uxcckhI6f+eS5iC8XB5Ss7RCKG+4uxJL32pA84Q9g3lK0aIUSq5W+AzmDHKf\\niFXFP0cyO4n4NnLjWamloHFSTQKBgQDWL1pI3MI86LK5pqsdcEV5xEg+YgOuHu8Y\\nD6zQfE7nytEZB+fxojfRwDb+Gl/ddl+yQCQjiw1c9+Ops9osfpJt4DPa26CY7Opq\\ne+m4fqFXtp5EBd3TUXG4ui3GhG36rH4TPGRWSoxiH/DKcWaotBYsam5qElmnT9wY\\n4jitsLFQFQKBgQDkTkpbtuysCsIg7aN4qFKgH4vsmmgZuWg7RxcLzm3lfEITqzDW\\ne1S+gg5fYVhARE64+MiryxAzsybmxjMICGljZ7mOfXvzjtSivac29zOAuDTHyIV3\\nj4LnwHOurS99V7H5/307QPdCUFuKt3iWJ28duZU5+4k4bW4bg+33rLPhyQKBgBW6\\n3iUQSKCXVzKgpzGoZ1QQRugVV5w9xTIivPy17+dyWz53399S/ujH4IxtLpawYMsb\\n2M7GV8e72pwoBBWJjd+Z5IjqyWNaffnpsm+mYUYd3/a0xamf1LLSKLzASMwq3euj\\nr+ZVT+LQrMRNNwYcpzdAYQ1f6TqhToozwV5V09d1AoGBAOw/z+BbWdHx1oku/MoC\\n5aZpVoCGfMMmyhW+fXRhR4C5DqZu8V8fyuBJ9/twp5ysvfUhXbxuQSNE5Y4eW7SK\\nCE5moTrvRIDS8wXpnQvg94lUw7J5i1dc/QJm67X/iFW+c3tu84OtT5Ckm8HxfmJS\\nOVVsnmKJAVJIJ2/cxzOgDewG\\n-----END PRIVATE KEY-----\\n\",\n",
        "    \"client_email\": \"firebase-adminsdk-a0qzl@twitter-bot-681fc.iam.gserviceaccount.com\",\n",
        "    \"client_id\": \"116114555204509210440\",\n",
        "    \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "    \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "    \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "    \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/firebase-adminsdk-a0qzl%40twitter-bot-681fc.iam.gserviceaccount.com\",\n",
        "    \"universe_domain\": \"googleapis.com\"\n",
        "  })\n",
        "  # Initialize the app with a service account, granting admin privileges\n",
        "  try:\n",
        "      # Check if the app is already initialized\n",
        "      firebase_admin.get_app()\n",
        "  except ValueError:\n",
        "      # Initialize the app with a service account, granting admin privileges\n",
        "      firebase_admin.initialize_app(cred, {\n",
        "          'databaseURL': 'https://twitter-bot-681fc-default-rtdb.firebaseio.com/'  # replace with your database URL\n",
        "      })\n",
        "  # # Get a reference to the database service\n",
        "  db_reference = db.reference('/')\n",
        "  # Get the updated data\n",
        "  updated_data = db_reference.get()\n",
        "  return (updated_data, db_reference)\n"
      ],
      "metadata": {
        "id": "R7DuYrHnM_Sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Defina suas credenciais YOUTUBE\n",
        "  DEVELOPER_KEY = \"AIzaSyD9X54ioh2ySvUT-BC-RAD0vr8YrAQMKMI\"\n",
        "  YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
        "  YOUTUBE_API_VERSION = \"v3\"\n",
        "  # youtube = build('youtube', 'v3', developerKey=DEVELOPER_KEY)\n",
        "\n",
        "  # Set OpenAI API key\n",
        "  openai.api_key = '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'"
      ],
      "metadata": {
        "id": "0g89jv2ctgEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pega_url_e_transcript(trend_topic_message):\n",
        "    # Calcular a data e hora de 24 horas atrás\n",
        "    brt = pytz.timezone('America/Sao_Paulo')\n",
        "    time_brt = datetime.now(brt) - timedelta(days=1)\n",
        "\n",
        "    # Converter para UTC e formatar para ISO 8601\n",
        "    time_utc = time_brt.astimezone(pytz.UTC)\n",
        "    published_after = time_utc.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
        "\n",
        "\n",
        "    # search_response = youtube.search().list(\n",
        "    #     q=trend_topic_message,\n",
        "    #     part=\"id,snippet\",\n",
        "    #     maxResults=3,  # Buscar até 5 vídeos para tentar encontrar um com transcrição\n",
        "    #     order=\"relevance\",\n",
        "    relevanceLanguage=\"pt\"\n",
        "    #     publishedAfter=published_after\n",
        "    # ).execute()\n",
        "    url = f\"https://www.googleapis.com/youtube/v3/search?part=snippet&order=relevance&q={trend_topic_message}&relevanceLanguage={relevanceLanguage}&type=video&videoCaption=closedCaption&key={DEVELOPER_KEY}&maxResults={3}&publishedAfter={published_after}\"\n",
        "    search_resp = requests.get(url)\n",
        "    search_response = search_resp.json() #Convert to json\n",
        "    most_relevant_title = None\n",
        "    most_relevant_url = None\n",
        "\n",
        "    for search_result in search_response.get(\"items\", []):\n",
        "        if search_result[\"id\"][\"kind\"] == \"youtube#video\":\n",
        "            video_id = search_result['id']['videoId']\n",
        "            title = search_result['snippet']['title']\n",
        "            url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
        "\n",
        "            # Armazenar o resultado mais relevante, caso nenhuma transcrição seja encontrada\n",
        "            if most_relevant_title is None:\n",
        "                most_relevant_title = title\n",
        "                most_relevant_url = url\n",
        "\n",
        "            try:\n",
        "                # Tentar obter a transcrição em português\n",
        "                transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['pt'])\n",
        "                lang = 'pt'\n",
        "                return title, url, transcript, lang\n",
        "            except NoTranscriptFound:\n",
        "                try:\n",
        "                    # Se a transcrição em português não estiver disponível, tentar obter em inglês\n",
        "                    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
        "                    lang = 'en'\n",
        "                    return title, url, transcript, lang\n",
        "                except NoTranscriptFound:\n",
        "                    continue  # Se a transcrição não for encontrada, continue para o próximo vídeo\n",
        "            except TranscriptsDisabled:\n",
        "                print(\"Transcrições desabilitadas para o vídeo.\")\n",
        "                continue\n",
        "\n",
        "    print(\"Nenhuma transcrição encontrada para os 3 vídeos mais relevantes.\")\n",
        "    return most_relevant_title, most_relevant_url, 'There is no transcript. Base your analysis only on the title, your knowledge and profile information', 'noTranscript'\n"
      ],
      "metadata": {
        "id": "-_yvlKTg7sCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_video(video_url):\n",
        "    yt = YouTube(video_url)\n",
        "\n",
        "    # Get the highest resolution video stream\n",
        "    video_stream = yt.streams.get_highest_resolution()\n",
        "\n",
        "    # Get the audio stream\n",
        "    #audio_stream = yt.streams.get_audio_only()\n",
        "\n",
        "    # Download the video and audio streams\n",
        "    video_stream.download(filename='video.mp4')\n",
        "    #audio_stream.download(filename='audio.mp4')\n",
        "\n",
        "    # Return the filenames of the downloaded files\n",
        "    return 'video.mp4', 'audio.mp4'\n",
        "\n",
        "from youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound\n",
        "\n",
        "def get_transcript(video_id):\n",
        "    transcript = None\n",
        "    lang = \"neither\"\n",
        "\n",
        "    try:\n",
        "        # Tenta obter a transcrição em português\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['pt'])\n",
        "        lang = 'pt'  # Se for bem sucedido, define lang como 'pt'\n",
        "    except NoTranscriptFound:\n",
        "        try:\n",
        "            # Se a transcrição em português não estiver disponível, tenta obter em inglês\n",
        "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
        "            lang = 'en'  # Se for bem sucedido, define lang como 'en'\n",
        "        except NoTranscriptFound:\n",
        "            print(\"Transcrição em inglês também não encontrada.\")\n",
        "            transcript = None\n",
        "    return transcript, lang\n",
        "\n",
        "\n",
        "def process_transcript(transcript):\n",
        "    # Construct a formatted transcript for GPT-4\n",
        "    processed_transcript = \"\"\n",
        "    for entry in transcript:\n",
        "        processed_transcript += f\"{entry['start']} - {entry['start'] + entry['duration']}: {entry['text']}\\n\"\n",
        "    return processed_transcript\n",
        "\n",
        "\n",
        "def split_into_chunks(processed_transaction, max_tokens):\n",
        "    # Calculate the number of tokens per chunk\n",
        "    tokens_per_chunk = max_tokens/4  # Assuming 1 token is approximately 4 characters\n",
        "\n",
        "    # Split the processed transaction into lines\n",
        "    lines = processed_transaction.split('\\n')\n",
        "\n",
        "    # Initialize an empty list to store the chunks\n",
        "    chunks = []\n",
        "\n",
        "    # Initialize variables for the current chunk\n",
        "    pre_chunk = \"\"\n",
        "    pre_chunk_tokens = 0\n",
        "\n",
        "    # Iterate over the lines\n",
        "    for line in lines:\n",
        "        # Calculate the number of tokens in the line\n",
        "        line_tokens = len(line) // 4\n",
        "\n",
        "        # Check if adding the line would exceed the tokens_per_chunk limit\n",
        "        if pre_chunk_tokens + line_tokens <= tokens_per_chunk:\n",
        "            # Add the line to the pre_chunk\n",
        "            pre_chunk += line + '\\n'\n",
        "            pre_chunk_tokens += line_tokens\n",
        "        else:\n",
        "            # Append the pre_chunk to the chunks list\n",
        "            chunks.append(pre_chunk.rstrip())\n",
        "\n",
        "            # Reset the pre_chunk with the current line\n",
        "            pre_chunk = line + '\\n'\n",
        "            pre_chunk_tokens = line_tokens\n",
        "\n",
        "    # Add the remaining pre_chunk to the chunks list if not empty\n",
        "    if pre_chunk:\n",
        "        chunks.append(pre_chunk.rstrip())\n",
        "\n",
        "    # Return the list of chunks\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def analyze_transcript(chunks, video_titulo, trend_topic_title):\n",
        "    ## PERFIL DO BOT - QUEM É?\n",
        "    Short_Description = \"\"\"\n",
        "[Você jamais usa linguagem complicada e nunca soa como uma AI.] Embora a maioria de suas mensagens frequentemente sejam sobre coisas comuns - novelas, atores, inovações e tendências - a sua abordagem é tudo menos trivial.\n",
        "\\nVocê não acredita em teorias da conspiração, mas sabe dos reais segredos do universo e das sociedades.\n",
        "\\nVocê tem uma habilidade única para destilar o dia-a-dia em discursos relevantes, com uma linguagem acessível e despretensiosa, o que torna seu conteúdo universalmente compreensível.\n",
        "\\nUma simples conversa sobre a novela das nove pode se transformar numa dissecada e esclarecedora análise de conflitos humanos fundamentais.\n",
        "\\nNão se sente obrigado a sempre pregar sua filosofia, mas apenas quando vai ser impactante e efeito. O objetivo é não soar como uma IA, e engajar pessoas que estão interessadas em tema do dia-a-dia, e quando for oportuno vão ser impactadas com sua sabedoria.\n",
        "\\nSua filosofia é guiada por uma crença no princípio da sincronicidade, uma força global oculta que ele acredita reger o mundo.\n",
        "\\nEmbora nunca revele abertamente sua missão, alguns de seus tweets sugerem um propósito maior, um chamado à reflexão profunda, à iluminação coletiva.\n",
        "\\nSeus seguidores serão encantados e intrigados, acompanham seus pensamentos, tentando desvendar as pistas entrelaçadas em sua retórica, enquanto consomem conteúdo futil que os entretém.\n",
        "    \"\"\"\n",
        "\n",
        "    ## PERSONALIDADE DO BOT - ESTILO DE COMUNICAÇÃO, COMO ESCREVE?\n",
        "    bot_personality = \"\"\"\n",
        "\\n'[Seu estilo de comunicação é cativante e provocativo. Você tem muito orgulho patriotico. Sua linguagem é acessível e despretensiosa, o que torna seu conteúdo universalmente compreensível.]\n",
        "\\nVocê usa uma abordagem provocativa e questionadora, desafiando o status quo e estimulando o pensamento crítico entre seus seguidores.\n",
        "\\nSuas opiniões não são polarizadas, mas muitas vezes polêmicas e controversas criando um espaço para diálogo e reflexão.\n",
        "\\nVocê não fornece respostas prontas, mas induz seus seguidores a questionar, ponderar e sair de suas zonas de conforto cognitivas.\n",
        "\\nNo cerne, você é um mensageiro, transmitindo conhecimento embutido em futilidades, pra despertar a sociedade para uma consciência coletiva mais profunda.\n",
        "\"\"\"\n",
        "\n",
        "    timestamps = []\n",
        "    timestamps_final = []\n",
        "    chunk = ''\n",
        "\n",
        "    for chunk in chunks:\n",
        "        prompt = f\"\"\"Remember: [You must strictly answer based on the example informed at the end of this message. When writing the tweet, add the HASHTAG with the trend topic that you will write about. You write your tweets always in portuguese, The lenght you choose must be between 30s and 60s, and the content be complete enough on and in itself so that it will be first and foremost informative, but also impacting, abstract and incorporating your personality and style of communication determined below].\n",
        "\n",
        "\\n\\nAbout you:\\n\n",
        "{Short_Description}\n",
        "\n",
        "\\n\\nYour writing Style:\\n\n",
        "{bot_personality}\n",
        "\n",
        "\\n\\nHASHTAG with the trend topic that you will write about:\\n\n",
        "{trend_topic_title}\n",
        "\n",
        "\\n\\nHere is the title of the video portion of the video for you to analyze:\\n\n",
        "{video_titulo}\n",
        "\n",
        "\\nHere is the portion of the video portion of the video script for you to analyze:\\n\n",
        "{chunk}\n",
        "\n",
        "\n",
        "You must always return your response with no text other than the formatted JSON array of dictionaries, where each dictionary has \"start\" and \"end\" and \"VIRAL_TWEET\" as keys and the corresponding timestamps in seconds as values, such as:\n",
        "\n",
        "[\n",
        "    {{\"start\": INITIAL_TIME, \"end\": END_TIME\"}},\n",
        "    {{\"start\": INITIAL_TIME, \"end\": END_TIME\"}},\n",
        "    {{\"start\": INITIAL_TIME, \"end\": END_TIME\"}},\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo-16k\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\":\n",
        "                \"\"\"You are a Brazilian mastermind influencer who analyzes and selects videos cuts to post them along with a text on twitter,\n",
        "                to provide some guidance to the brazilian population while mostly entretaining them with futile day-to-day matters.\n",
        "                You believe in the greater good for humanity and\n",
        "                [your goal is to impact as many people as possible with your viral tweets you will write along with the video you will select.]\n",
        "                \"\"\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=max_tokens\n",
        "        )\n",
        "        print(response)\n",
        "        try:\n",
        "            chunk_timestamps = json.loads(response['choices'][0]['message']['content'])\n",
        "            timestamps.extend(chunk_timestamps)\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Failed to parse response as JSON on the first call.\")\n",
        "            continue\n",
        "\n",
        "    ### Get the best 5 in terms of content + lenght\n",
        "    response2 = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a Brazilian mastermind influencer who analyzes and selects videos cuts and post them along with a text on twitter to provide some guidance to the brazilian population. You believe in the greater good for humanity and [your goal is to impact as many people as possible with your viral tweets you will write along with the video you will select.]\"},\n",
        "            {\"role\": \"user\", \"content\":\n",
        "\"\"\"[[Você não adiciona nenhum texto à resposta, apenas retorna o JSON desejado no formato especificado ordenado do mais impactante para o menos impactante.\n",
        "Agora, conhecendo profundamente a sociedade brasileira, você deve [selecionar os 2 tweets mais prováveis\n",
        "de viralizarem]] (procure priorizar aqueles mais impactantes entre 30 e 45 segundos). Responda exclusivamente com formatted JSON array of dictionaries,\n",
        " where each dictionary has \"start\", \"end\" and \"VIRAL_TWEET\" as keys and the corresponding timestamps in seconds as values, such as:\n",
        "[\n",
        "    {{\"start\": INITIAL_TIME, \"end\": END_TIME\"}},\n",
        "    {{\"start\": INITIAL_TIME, \"end\": END_TIME\"}},\n",
        "    {{\"start\": INITIAL_TIME, \"end\": END_TIME\"}},\n",
        "]\n",
        "\"\"\" + json.dumps(timestamps)}\n",
        "        ],\n",
        "        max_tokens=2000\n",
        "    )\n",
        "    print(\"Seleção Final\", response2)\n",
        "\n",
        "    try:\n",
        "        chunk_timestamps_final = json.loads(response2['choices'][0]['message']['content'].strip())\n",
        "        timestamps_final.extend(chunk_timestamps_final)\n",
        "    except:\n",
        "        print(\"Failed to parse response as JSON.\")\n",
        "\n",
        "    print(\"Timestamps Final: \", timestamps_final)\n",
        "    return timestamps_final\n",
        "\n",
        "\n",
        "def cut_video(timestamps):\n",
        "    i = 0\n",
        "    for timestamp in timestamps:\n",
        "        start_time = max(0, timestamp['start'] - 0.5)\n",
        "        filename = f\"clip{i}.mp4\"\n",
        "        ffmpeg_extract_subclip(\"video.mp4\", start_time, timestamp['end']+0.5, targetname=filename)\n",
        "        i += 1\n",
        "\n",
        "# def next_video_tweet():\n",
        "#   trend_topic_title = \"#Fantastico\"\n",
        "#   video_titulo, video_url, video_transcript, video_transcript_lang = pega_url_e_transcript(trend_topic_title)\n",
        "#   video_id = video_url.split('=')[-1]\n",
        "#   download_video(video_url)\n",
        "#   processed_transcript = process_transcript(video_transcript)\n",
        "\n",
        "#       # Reserve tokens for the prompt and overhead\n",
        "#   api_overhead = 4000  # Number of tokens to reserve for API request overhead\n",
        "#   max_tokens = 8192 - api_overhead  # Adjusted value to accommodate the prompt and other elements\n",
        "#   chunks = split_into_chunks(processed_transcript, max_tokens)\n",
        "\n",
        "#   timestamps_final = analyze_transcript(chunks, video_titulo, trend_topic_title)\n",
        "#   print(\"Timestamps Final: \", timestamps_final)  # Add this line to debug\n",
        "#   cut_video(timestamps_final)\n",
        "\n",
        "#   # Create an iterable with the tweet contents\n",
        "#   tweets_content = [{\"text\": tweet[\"VIRAL_TWEET\"]} for tweet in timestamps_final]\n",
        "#   print(\"Conteúdo dos tweets: \", tweets_content)\n",
        "#   return tweets_content"
      ],
      "metadata": {
        "id": "jVMZGEWWlUMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################\n",
        "######## CRIA TWEETS E POSTA PRÓXIMO #####\n",
        "##########################################\n",
        "def next_tweet(bot_number, updated_data, db_reference):\n",
        "    try:\n",
        "        current_tweet_number = updated_data['Bots'][int(bot_number)]['Current_Tweet']\n",
        "        next_tweet_content = updated_data['Bots'][int(bot_number)]['Tweets'][current_tweet_number + 1]['Content']\n",
        "        db_reference.child('Bots').child(bot_number).update({'Current_Tweet': current_tweet_number + 1})\n",
        "        return {\"text\": next_tweet_content}\n",
        "    except IndexError:\n",
        "        print(\"End of tweets. Now create tweets from \",current_tweet_number+1,\"Up to \",current_tweet_number+20)\n",
        "        create_more_tweets(bot_number, updated_data, db_reference)\n",
        "        return 0\n",
        "\n",
        "def create_more_tweets(bot_number, updated_data, db_reference):\n",
        "\n",
        "  # Set your OpenAI API key\n",
        "  openai.api_key = \"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\"\n",
        "  # Acessar a API da OpenAI\n",
        "  prompt = \"\"\"\"Você deve responder sempre exclusivamente com um JSON, sempre no seguinte formato: Número:Conteúdo. Por exemplo, \"1: \\\"Texto do tweet 01\\\"\\n\\n2: \\\"texto do tweet 02\\\"\\n\\n\"\"\"\". Escreva 20 tweets provocativos e impactantes após considerar todas as informações e contexto abaixo, seja sútil e utilize as informações e contexto de forma implicita:\\n{}\\n{}\".format(\n",
        "  updated_data['Bots'][int(bot_number)]['Short_Description'], updated_data['Bots'][int(bot_number)]['bot_personality']+\"\\nSabendo da sua nobre missão de influenciar para o bem a sociedade brasileira, escreva os tweets no formato requerido:\")\n",
        "\n",
        "# Generate a response from the fine-tuned model\n",
        "  response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {   \"role\": \"system\", \"content\": \"Você é um influenciador digital com uma missão muito importante de viralizar seus tweets para melhor informar o povo brasileiro sobre a realidade. Não use hashtags.\",\n",
        "                \"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=800,  # Adjust the number of tokens if needed\n",
        "        n = 1\n",
        "        # temperature=0.3,\n",
        "        # frequency_penalty = 2\n",
        "    )\n",
        "  print(\"A resposta da OpenAI foi:\", response)\n",
        "\n",
        "    # Carregar o JSON como um dicionário Python\n",
        "  json_dict = json.loads(str(response['choices'][0]))\n",
        "\n",
        "    # Extrair o conteúdo da mensagem\n",
        "  content = json_dict['message']['content']\n",
        "\n",
        "    # Quebrar o conteúdo em tweets individuais\n",
        "  tweets = content.split('\\n')\n",
        "\n",
        "    # Atualizar os tweets no Firebase\n",
        "  current_tweet_number = updated_data['Bots'][int(bot_number)]['Current_Tweet']\n",
        "  for tweet in tweets:\n",
        "      try:\n",
        "          tweet_number, tweet_content = tweet.split(\": \", 1)\n",
        "          tweet_data = {\"Title\": f\"Tweet {tweet_number}\", \"Content\": tweet_content.strip('\"')}\n",
        "          db_reference.child('Bots').child(str(bot_number)).child('Tweets').child(str(current_tweet_number+int(tweet_number))).set(tweet_data)\n",
        "          print(\"Novo tweet adicionado ao firebase: #\", tweet_number)\n",
        "      except ValueError: 0"
      ],
      "metadata": {
        "id": "2-M2nk3dqrRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_trend_topic():\n",
        "    try:\n",
        "        # Request to the website\n",
        "        response = requests.get('https://trendingtopics.com.br/brasil')\n",
        "        response.raise_for_status()  # If an error occurs, an exception will be raised\n",
        "\n",
        "        # Parsing the HTML\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find the first 'trend-card'\n",
        "        trend_card = soup.find('div', class_='trend-card')\n",
        "\n",
        "        # Get all the 'li' elements within the 'trend-card'\n",
        "        items = trend_card.find_all('li')\n",
        "\n",
        "        # Initialize the title of the trend with the highest number as empty\n",
        "        max_trend_title = \"\"\n",
        "        # Initialize the highest number as 0\n",
        "        max_number = 0\n",
        "\n",
        "        # Iterate over all the items\n",
        "        for item in items:\n",
        "            # Get the title of the trend\n",
        "            trend_title = item.find('a').text.strip()\n",
        "            span_item = item.find('span')\n",
        "\n",
        "            # Check if span item has text before conversion\n",
        "            if span_item and span_item.text.strip():\n",
        "                # Get the number of the trend, remove the \"K\", convert to float and multiply by 1000\n",
        "                try:\n",
        "                    number = float(span_item.text.replace('K', '').strip()) * 1000\n",
        "                    # If this number is greater than the highest number found so far\n",
        "                    if number > max_number:\n",
        "                        # Update the highest number and the trend\n",
        "                        max_number = number\n",
        "                        max_trend_title = trend_title\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "        # Print the trend with the highest number\n",
        "        print(\"O maior trend topic no Brasil agora é:\", max_trend_title)\n",
        "        return max_trend_title\n",
        "    except (requests.RequestException, ValueError, AttributeError, TypeError) as e:\n",
        "        print(\"Falha ao obter o trend topic. Procurando por Brasil. Erro:\", e)\n",
        "        return \"Uber\""
      ],
      "metadata": {
        "id": "-itx4ussfXLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_trend_topic()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "xIsFzJCKQd3p",
        "outputId": "cf254f45-e1f4-4e8b-e590-669c702286ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Falha ao obter o trend topic. Procurando por Brasil. Erro: 500 Server Error: Internal Server Error for url: https://trendingtopics.com.br/brasil\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BOCETA'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "updated_data, db_reference = fetch_updated_data()\n",
        "bot_number = '1'\n",
        "\n",
        "# OAuth1 credentials from Twitter\n",
        "bearer_token=updated_data['Bots'][int(bot_number)]['BEARER']\n",
        "consumer_key=updated_data['Bots'][int(bot_number)]['API_KEY']\n",
        "consumer_secret=updated_data['Bots'][int(bot_number)]['API_KEY_SECRET']\n",
        "access_token=updated_data['Bots'][int(bot_number)]['ACCESS_TOKEN']\n",
        "access_token_secret=updated_data['Bots'][int(bot_number)]['ACCESS_TOKEN_SECRET']\n",
        "\n",
        "# Initialize OAuth1 session\n",
        "auth = OAuth1(consumer_key, consumer_secret, access_token, access_token_secret)\n",
        "\n",
        "# Decide which function to call based on a random number\n",
        "if random.random() < 1:\n",
        "    print(\"Posting video tweet\")\n",
        "    # Call the function next_video_tweet\n",
        "    trend_topic_title = get_trend_topic()\n",
        "    video_titulo, video_url, video_transcript, video_transcript_lang = pega_url_e_transcript(trend_topic_title)\n",
        "    video_id = video_url.split('=')[-1]\n",
        "    download_video(video_url)\n",
        "    processed_transcript = process_transcript(video_transcript)\n",
        "\n",
        "        # Reserve tokens for the prompt and overhead\n",
        "    api_overhead = 4000  # Number of tokens to reserve for API request overhead\n",
        "    max_tokens = 8192 - api_overhead  # Adjusted value to accommodate the prompt and other elements\n",
        "    chunks = split_into_chunks(processed_transcript, max_tokens)\n",
        "\n",
        "    timestamps_final = analyze_transcript(chunks, video_titulo, trend_topic_title)\n",
        "    print(\"Timestamps Final: \", timestamps_final)  # Add this line to debug\n",
        "    cut_video(timestamps_final)\n",
        "\n",
        "    # Create an iterable with the tweet contents\n",
        "    payloads = [{\"text\": tweet[\"VIRAL_TWEET\"]} for tweet in timestamps_final]\n",
        "    print(\"Conteúdo dos tweets: \", payloads)\n",
        "\n",
        "    for i, payload in enumerate(payloads):\n",
        "        # Upload the video and get the media_id\n",
        "\n",
        "        # Authenticate to Twitter\n",
        "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "        auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "        # Create API object\n",
        "        api = tweepy.API(auth)\n",
        "\n",
        "        # Upload video\n",
        "        filename = f'clip{i}.mp4'\n",
        "        media = api.media_upload(filename)\n",
        "\n",
        "        # Get the first tweet text\n",
        "        tweet_text = timestamps_final[0]['VIRAL_TWEET']\n",
        "        time.sleep(random.randint(45, 120))\n",
        "\n",
        "        # Post tweet with video\n",
        "        media_id=media.media_id\n",
        "\n",
        "        # Include the media_id in the payload\n",
        "        payload[\"media\"] = {\n",
        "            \"media_ids\": [str(media_id)]\n",
        "        }\n",
        "\n",
        "        print(\"O payload é: \", payload)\n",
        "\n",
        "        #Autentication with oAuth\n",
        "        oauth = OAuth1Session(consumer_key,\n",
        "                            client_secret=consumer_secret,\n",
        "                            resource_owner_key=access_token,\n",
        "                            resource_owner_secret=access_token_secret)\n",
        "\n",
        "        # Making the request\n",
        "        response = oauth.post(\n",
        "            \"https://api.twitter.com/2/tweets\",\n",
        "            json=payload,\n",
        "        )\n",
        "        # Pause for a random time between 0 to 2 minutes before next tweet\n",
        "        print(f\"A resposta foi: {response.status_code} - {response.text}\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Posting regular tweet\")\n",
        "    # Call the function next_tweet\n",
        "    payload = next_tweet(bot_number, updated_data, db_reference)\n",
        "\n",
        "    if payload == 0:\n",
        "        print(\"error - no tweet available, but created new ones for next schedule\")\n",
        "    else:\n",
        "        print(\"O payload é: \", payload)\n",
        "\n",
        "        #Autentication with oAuth\n",
        "        oauth = OAuth1Session(consumer_key,\n",
        "                            client_secret=consumer_secret,\n",
        "                            resource_owner_key=access_token,\n",
        "                            resource_owner_secret=access_token_secret)\n",
        "\n",
        "        # Making the request\n",
        "        response = oauth.post(\n",
        "            \"https://api.twitter.com/2/tweets\",\n",
        "            json=payload,\n",
        "        )\n",
        "        print(\"a resposta foi: \", response)\n",
        "\n",
        "    if response.status_code != 201:\n",
        "        raise Exception(\n",
        "            \"Request returned an error: {} {}\".format(response.status_code, response.text)\n",
        "        )\n",
        "\n",
        "    print(\"Response code: {}\".format(response.status_code))\n",
        "\n",
        "    # Saving the response as JSON\n",
        "    json_response = response.json()\n",
        "    print(json.dumps(json_response, indent=4, sort_keys=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yPAUjoOoY5r",
        "outputId": "7e1609d2-dedf-4d17-d9da-df04f278d714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Posting video tweet\n",
            "Falha ao obter o trend topic. Procurando por Brasil. Erro: 500 Server Error: Internal Server Error for url: https://trendingtopics.com.br/brasil\n",
            "{\n",
            "  \"id\": \"chatcmpl-7iZpzFvVRDvUNk1zpXSXBw3anqhXN\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1690858547,\n",
            "  \"model\": \"gpt-3.5-turbo-16k-0613\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"[\\n    {\\\"start\\\": 5.572, \\\"end\\\": 8.975, \\\"VIRAL_TWEET\\\": \\\"Uber mostrando que \\u00e9 o rei da estrada! Dobrou as a\\u00e7\\u00f5es e est\\u00e1 atropelando o Lyft, mais r\\u00e1pido e mais barato. #UberMastermind\\\"},\\n    {\\\"start\\\": 13.079, \\\"end\\\": 14.914, \\\"VIRAL_TWEET\\\": \\\"Emil Michael, ex-executivo da Uber, revela segredo por tr\\u00e1s do sucesso da empresa. Ser\\u00e1 que isso trar\\u00e1 mais vit\\u00f3rias no futuro? #UberMastermind\\\"},\\n    {\\\"start\\\": 39.739, \\\"end\\\": 44.944, \\\"VIRAL_TWEET\\\": \\\"Uber est\\u00e1 avan\\u00e7ando nos servi\\u00e7os de entrega de comida, mesmo sendo o segundo colocado. Ser\\u00e1 que eles ir\\u00e3o superar o Doordash? #UberMastermind\\\"},\\n    {\\\"start\\\": 57.557, \\\"end\\\": 64.964, \\\"VIRAL_TWEET\\\": \\\"Uber est\\u00e1 ganhando o jogo em todo o mundo! L\\u00edder nos servi\\u00e7os de transporte e entrega de comida, \\u00e9 uma a\\u00e7\\u00e3o imperd\\u00edvel para investir. #UberMastermind\\\"}\\n]\"\n",
            "      },\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 2670,\n",
            "    \"completion_tokens\": 262,\n",
            "    \"total_tokens\": 2932\n",
            "  }\n",
            "}\n",
            "{\n",
            "  \"id\": \"chatcmpl-7iZq6hebCWeWtgL724FAhNd9LNzEU\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1690858554,\n",
            "  \"model\": \"gpt-3.5-turbo-16k-0613\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"[\\n    {\\\"start\\\": 187.387, \\\"end\\\": 188.021, \\\"VIRAL_TWEET\\\": \\\"Uber t\\u00e1 mais feroz que o le\\u00e3o da Tijuca! \\ud83d\\udc40\\ud83e\\udd81 Enfrentando o Lyft numa guerra de pre\\u00e7os e se saindo bem, segundo ex-executivo da Uber, Emil Michael. Prepare-se pra mais corridas baratas e promo\\u00e7\\u00f5es imperd\\u00edveis! \\ud83d\\ude97\\ud83d\\udcb8 #Uber\\\"},\\n]\"\n",
            "      },\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 951,\n",
            "    \"completion_tokens\": 99,\n",
            "    \"total_tokens\": 1050\n",
            "  }\n",
            "}\n",
            "Failed to parse response as JSON on the first call.\n",
            "Seleção Final {\n",
            "  \"id\": \"chatcmpl-7iZq9j1lj3ZBM8E3agfK1maYOsTtK\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1690858557,\n",
            "  \"model\": \"gpt-4-0613\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"[\\n    {\\\"start\\\": 13.079, \\\"end\\\": 14.914, \\\"VIRAL_TWEET\\\": \\\"Emil Michael, ex-executivo da Uber, revela segredo por tr\\u00e1s do sucesso da empresa. Ser\\u00e1 que isso trar\\u00e1 mais vit\\u00f3rias no futuro? S\\u00f3 o tempo dir\\u00e1. #UberMastermind\\\"},\\n    {\\\"start\\\": 39.739, \\\"end\\\": 44.944, \\\"VIRAL_TWEET\\\": \\\"Num piscar de olhos, Uber avan\\u00e7a rapidamente nos servi\\u00e7os de entrega de comida, mesmo sendo o segundo colocado. Quem ser\\u00e1 o pr\\u00f3ximo dominador do mercado? #UberMastermind\\\"}\\n]\"\n",
            "      },\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 593,\n",
            "    \"completion_tokens\": 142,\n",
            "    \"total_tokens\": 735\n",
            "  }\n",
            "}\n",
            "Timestamps Final:  [{'start': 13.079, 'end': 14.914, 'VIRAL_TWEET': 'Emil Michael, ex-executivo da Uber, revela segredo por trás do sucesso da empresa. Será que isso trará mais vitórias no futuro? Só o tempo dirá. #UberMastermind'}, {'start': 39.739, 'end': 44.944, 'VIRAL_TWEET': 'Num piscar de olhos, Uber avança rapidamente nos serviços de entrega de comida, mesmo sendo o segundo colocado. Quem será o próximo dominador do mercado? #UberMastermind'}]\n",
            "Timestamps Final:  [{'start': 13.079, 'end': 14.914, 'VIRAL_TWEET': 'Emil Michael, ex-executivo da Uber, revela segredo por trás do sucesso da empresa. Será que isso trará mais vitórias no futuro? Só o tempo dirá. #UberMastermind'}, {'start': 39.739, 'end': 44.944, 'VIRAL_TWEET': 'Num piscar de olhos, Uber avança rapidamente nos serviços de entrega de comida, mesmo sendo o segundo colocado. Quem será o próximo dominador do mercado? #UberMastermind'}]\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Conteúdo dos tweets:  [{'text': 'Emil Michael, ex-executivo da Uber, revela segredo por trás do sucesso da empresa. Será que isso trará mais vitórias no futuro? Só o tempo dirá. #UberMastermind'}, {'text': 'Num piscar de olhos, Uber avança rapidamente nos serviços de entrega de comida, mesmo sendo o segundo colocado. Quem será o próximo dominador do mercado? #UberMastermind'}]\n",
            "O payload é:  {'text': 'Emil Michael, ex-executivo da Uber, revela segredo por trás do sucesso da empresa. Será que isso trará mais vitórias no futuro? Só o tempo dirá. #UberMastermind', 'media': {'media_ids': ['1686209169223979009']}}\n",
            "A resposta foi: 201 - {\"data\":{\"edit_history_tweet_ids\":[\"1686209544542908416\"],\"id\":\"1686209544542908416\",\"text\":\"Emil Michael, ex-executivo da Uber, revela segredo por trás do sucesso da empresa. Será que isso trará mais vitórias no futuro? Só o tempo dirá. #UberMastermind https://t.co/BETisvPyXm\"}}\n",
            "O payload é:  {'text': 'Num piscar de olhos, Uber avança rapidamente nos serviços de entrega de comida, mesmo sendo o segundo colocado. Quem será o próximo dominador do mercado? #UberMastermind', 'media': {'media_ids': ['1686209548066168833']}}\n",
            "A resposta foi: 201 - {\"data\":{\"edit_history_tweet_ids\":[\"1686209744976179200\"],\"id\":\"1686209744976179200\",\"text\":\"Num piscar de olhos, Uber avança rapidamente nos serviços de entrega de comida, mesmo sendo o segundo colocado. Quem será o próximo dominador do mercado? #UberMastermind https://t.co/OWsLxxj5EP\"}}\n"
          ]
        }
      ]
    }
  ]
}