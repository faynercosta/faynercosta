{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faynercosta/faynercosta/blob/main/Papers_Summary_commit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e67f200",
      "metadata": {
        "id": "3e67f200"
      },
      "source": [
        "# How to use functions with a knowledge base\n",
        "\n",
        "This notebook builds on the concepts in the [argument generation](How_to_call_functions_with_chat_models.ipynb) notebook, by creating an agent with access to a knowledge base and two functions that it can call based on the user requirement.\n",
        "\n",
        "We'll create an agent that uses data from arXiv to answer questions about academic subjects. It has two functions at its disposal:\n",
        "- **get_articles**: A function that gets arXiv articles on a subject and summarizes them for the user with links.\n",
        "- **read_article_and_summarize**: This function takes one of the previously searched articles, reads it in its entirety and summarizes the core argument, evidence and conclusions.\n",
        "\n",
        "This will get you comfortable with a multi-function workflow that can choose from multiple services, and where some of the data from the first function is persisted to be used by the second.\n",
        "\n",
        "## Walkthrough\n",
        "\n",
        "This cookbook takes you through the following workflow:\n",
        "\n",
        "- **Search utilities:** Creating the two functions that access arXiv for answers.\n",
        "- **Configure Agent:** Building up the Agent behaviour that will assess the need for a function and, if one is required, call that function and present results back to the agent.\n",
        "- **arXiv conversation:** Put all of this together in live conversation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "80e71f33",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80e71f33",
        "outputId": "becf811b-859c-479e-a2e5-bca230dc618b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (8.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken==0.3.3\n",
            "  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.3.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp (from openai)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.8 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting arxiv\n",
            "  Downloading arxiv-1.4.7-py3-none-any.whl (12 kB)\n",
            "Collecting feedparser (from arxiv)\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sgmllib3k (from feedparser->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=b6d56e600bb365a63aa2cc704a8981e8e98837a7e83c54586ff837743eea0fbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-1.4.7 feedparser-6.0.10 sgmllib3k-1.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scipy\n",
        "!pip install tenacity\n",
        "!pip install tiktoken==0.3.3\n",
        "!pip install termcolor\n",
        "!pip install openai\n",
        "!pip install requests\n",
        "!pip install arxiv\n",
        "!pip install pandas\n",
        "!pip install PyPDF2\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dab872c5",
      "metadata": {
        "id": "dab872c5"
      },
      "outputs": [],
      "source": [
        "import arxiv\n",
        "import ast\n",
        "import concurrent\n",
        "from csv import writer\n",
        "from IPython.display import display, Markdown, Latex\n",
        "import json\n",
        "import openai\n",
        "import os\n",
        "import pandas as pd\n",
        "from PyPDF2 import PdfReader\n",
        "import requests\n",
        "from scipy import spatial\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "import tiktoken\n",
        "from tqdm import tqdm\n",
        "from termcolor import colored\n",
        "import time\n",
        "\n",
        "GPT_MODEL = 'gpt-3.5-turbo-16k-0613'\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "openai.api_key = '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'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2e47962",
      "metadata": {
        "id": "f2e47962"
      },
      "source": [
        "## Search utilities\n",
        "\n",
        "We'll first set up some utilities that will underpin our two functions.\n",
        "\n",
        "Downloaded papers will be stored in a directory (we use ```./data/papers``` here). We create a file ```arxiv_library.csv``` to store the embeddings and details for downloaded papers to retrieve against using ```summarize_text```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "2de5d32d",
      "metadata": {
        "id": "2de5d32d"
      },
      "outputs": [],
      "source": [
        "def folder_setup():\n",
        "  # Set a directory to store downloaded papers\n",
        "  data_dir = os.path.join(os.curdir, \"data\", \"papers\")\n",
        "\n",
        "  # Create data/papers directory if it does not exist\n",
        "  if not os.path.exists(data_dir):\n",
        "      os.makedirs(data_dir)\n",
        "\n",
        "  paper_dir_filepath = \"./data/arxiv_library.csv\"\n",
        "\n",
        "  # Only generate a blank dataframe if the file does not exist\n",
        "  if not os.path.exists(paper_dir_filepath):\n",
        "    df = pd.DataFrame(list())\n",
        "    df.to_csv(paper_dir_filepath)\n",
        "  return 0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n"
      ],
      "metadata": {
        "id": "EqHngNcXwzfY"
      },
      "id": "EqHngNcXwzfY",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "57217b9d",
      "metadata": {
        "id": "57217b9d"
      },
      "outputs": [],
      "source": [
        "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
        "def embedding_request(text):\n",
        "    response = openai.Embedding.create(input=text, model=EMBEDDING_MODEL)\n",
        "    return response\n",
        "\n",
        "\n",
        "def get_articles(query, library=paper_dir_filepath, top_k=5):\n",
        "    \"\"\"This function gets the top_k articles based on a user's query, sorted by relevance.\n",
        "    It also downloads the files and stores them in arxiv_library.csv to be retrieved by the read_article_and_summarize.\n",
        "    \"\"\"\n",
        "    query=query\n",
        "    print(\"A query enviada para arxiv é: \", query)\n",
        "    search = arxiv.Search(\n",
        "        query=query, max_results=top_k, sort_by=arxiv.SortCriterion.Relevance\n",
        "    )\n",
        "    result_list = []\n",
        "    for result in search.results():\n",
        "        result_dict = {}\n",
        "        result_dict.update({\"title\": result.title})\n",
        "        result_dict.update({\"summary\": result.summary})\n",
        "\n",
        "        # Taking the first url provided\n",
        "        result_dict.update({\"article_url\": [x.href for x in result.links][0]})\n",
        "        result_dict.update({\"pdf_url\": [x.href for x in result.links][1]})\n",
        "        result_list.append(result_dict)\n",
        "\n",
        "        # Combine the title and summary\n",
        "        combined_text = result.title + ' ' + result.summary\n",
        "\n",
        "        # Request the embedding for the combined text instead of just the title\n",
        "        response = embedding_request(text=combined_text)\n",
        "\n",
        "        # Store references in library file\n",
        "        try:\n",
        "            file_reference = [\n",
        "                result.title,\n",
        "                result.download_pdf(data_dir),\n",
        "                response[\"data\"][0][\"embedding\"],\n",
        "            ]\n",
        "            #Write to file\n",
        "            with open(library, \"a\") as f_object:\n",
        "                writer_object = writer(f_object)\n",
        "                writer_object.writerow(file_reference)\n",
        "                f_object.close()\n",
        "            time.sleep(5)  # wait 5 seconds before the next download\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to download {result.title}. Error: {e}\")\n",
        "    return result_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dda02bdb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dda02bdb",
        "outputId": "1a772a61-ddfd-4db4-a537-6c6c341b0233"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'Hacia una teoria de unificacion para los comportamientos cognitivos',\n",
              " 'summary': \"Each cognitive science tries to understand a set of cognitive behaviors. The\\nstructuring of knowledge of this nature's aspect is far from what it can be\\nexpected about a science. Until now universal standard consistently describing\\nthe set of cognitive behaviors has not been found, and there are many questions\\nabout the cognitive behaviors for which only there are opinions of members of\\nthe scientific community. This article has three proposals. The first proposal\\nis to raise to the scientific community the necessity of unified the cognitive\\nbehaviors. The second proposal is claim the application of the Newton's\\nreasoning rules about nature of his book, Philosophiae Naturalis Principia\\nMathematica, to the cognitive behaviors. The third is to propose a scientific\\ntheory, currently developing, that follows the rules established by Newton to\\nmake sense of nature, and could be the theory to explain all the cognitive\\nbehaviors.\",\n",
              " 'article_url': 'http://arxiv.org/abs/0807.4680v3',\n",
              " 'pdf_url': 'http://arxiv.org/pdf/0807.4680v3'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Test that the search is working\n",
        "result_output = get_articles(\"terapia cognitivo comportamental\")\n",
        "result_output[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "11675627",
      "metadata": {
        "id": "11675627"
      },
      "outputs": [],
      "source": [
        "def strings_ranked_by_relatedness(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
        "    top_n: int = 100,\n",
        ") -> list[str]:\n",
        "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
        "    query_embedding_response = embedding_request(query)\n",
        "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
        "    strings_and_relatednesses = [\n",
        "        (row[\"filepath\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
        "        for i, row in df.iterrows()\n",
        "    ]\n",
        "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
        "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
        "    return strings[:top_n]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "7211df2c",
      "metadata": {
        "id": "7211df2c"
      },
      "outputs": [],
      "source": [
        "def read_pdf(filepath):\n",
        "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\"\"\"\n",
        "    # creating a pdf reader object\n",
        "    reader = PdfReader(filepath)\n",
        "    pdf_text = \"\"\n",
        "    page_number = 0\n",
        "    for page in reader.pages:\n",
        "        page_number += 1\n",
        "        pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
        "    return pdf_text\n",
        "\n",
        "\n",
        "# Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
        "def create_chunks(text, n, tokenizer):\n",
        "    \"\"\"Returns successive n-sized chunks from provided text.\"\"\"\n",
        "    tokens = tokenizer.encode(text)\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
        "        j = min(i + int(1.5 * n), len(tokens))\n",
        "        while j > i + int(0.5 * n):\n",
        "            # Decode the tokens and check for full stop or newline\n",
        "            chunk = tokenizer.decode(tokens[i:j])\n",
        "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
        "                break\n",
        "            j -= 1\n",
        "        # If no end of sentence found, use n tokens as the chunk size\n",
        "        if j == i + int(0.5 * n):\n",
        "            j = min(i + n, len(tokens))\n",
        "        yield tokens[i:j]\n",
        "        i = j\n",
        "\n",
        "\n",
        "def extract_chunk(content, template_prompt):\n",
        "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarize chunk of text\"\"\"\n",
        "    prompt = template_prompt + content\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=GPT_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
        "    )\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "# Summarize the top ranked PDF file, based on title + summary.\n",
        "def summarize_text(query):\n",
        "    \"\"\"This function does the following:\n",
        "    - Reads in the arxiv_library.csv file in including the embeddings\n",
        "    - Finds the closest file to the user's query\n",
        "    - Scrapes the text out of the file and chunks it\n",
        "    - Summarizes each chunk in parallel\n",
        "    - Does one final summary and returns this to the user\"\"\"\n",
        "\n",
        "    #Check if folders exist and create them if not\n",
        "    folder_setup()\n",
        "    # A prompt to dictate how the recursive summarizations should approach the input paper\n",
        "    summary_prompt = \"\"\"Summarize this text from an academic paper. Extract any key points with reasoning.\\n\\nContent:\"\"\"\n",
        "\n",
        "    # If the library is empty (no searches have been performed yet), we perform one and download the results\n",
        "    library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
        "    if len(library_df) == 0:\n",
        "        print(\"No papers searched yet, downloading first.\")\n",
        "        get_articles(query)\n",
        "        print(\"Papers downloaded, continuing\")\n",
        "        library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
        "    else: print(\"Search already done in the past, continuing\")\n",
        "    library_df.columns = [\"title\", \"filepath\", \"embedding\"]\n",
        "    library_df[\"embedding\"] = library_df[\"embedding\"].apply(ast.literal_eval)\n",
        "    strings = strings_ranked_by_relatedness(query, library_df, top_n=1)\n",
        "    print(\"Chunking text from paper\")\n",
        "    pdf_text = read_pdf(strings[0])\n",
        "\n",
        "    # Initialise tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    results = \"\"\n",
        "\n",
        "    # Chunk up the document into 1500 token chunks\n",
        "    chunks = create_chunks(pdf_text, 1500, tokenizer)\n",
        "    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
        "    print(\"Summarizing each chunk of text\")\n",
        "\n",
        "    # Parallel process the summaries\n",
        "    with concurrent.futures.ThreadPoolExecutor(\n",
        "        max_workers=len(text_chunks)\n",
        "    ) as executor:\n",
        "        futures = [\n",
        "            executor.submit(extract_chunk, chunk, summary_prompt)\n",
        "            for chunk in text_chunks\n",
        "        ]\n",
        "        with tqdm(total=len(text_chunks)) as pbar:\n",
        "            for _ in concurrent.futures.as_completed(futures):\n",
        "                pbar.update(1)\n",
        "        for future in futures:\n",
        "            data = future.result()\n",
        "            results += data\n",
        "\n",
        "    # Final summary\n",
        "    print(\"Summarizing into overall summary\")\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=GPT_MODEL,\n",
        "        messages=[\n",
        "            # {\n",
        "            #     \"role\": \"user\",\n",
        "            #     \"content\": f\"\"\"Write a summary collated from this collection of key points extracted from an academic paper.\n",
        "            #             The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
        "            #             User query: {query}\n",
        "            #             The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
        "            #             Key points:\\n{results}\\nSummary:\\n\"\"\",\n",
        "            # }\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"Escreva um resumo, como se fosse para um adolescente layman, composto por esta coleção de pontos chave extraídos de um artigo científico.\n",
        "                        O resumo deve pontuar o argumento chave, as conclusões, qualquer dado interessante e responder a pergunta do usuário.\n",
        "                        pergunta do usuário: {query}\n",
        "                        O resumo deve ser estruturado em uma lista de pontos seguindo os títulos Argumento chave, Evidências, Pontos interessantes e Conclusão.\n",
        "                        Pontos Chave:\\n{results}\\nResumo:\\n\"\"\",\n",
        "            }\n",
        "        ],\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "898b94d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "898b94d4",
        "outputId": "59650fe6-acac-4cb4-9cf2-2815de6937ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No papers searched yet, downloading first.\n",
            "A query enviada para arxiv é:  Terapia cognitivo comportamental\n",
            "Papers downloaded, continuing\n",
            "Chunking text from paper\n",
            "Summarizing each chunk of text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:08<00:00,  2.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarizing into overall summary\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-718b40e22753>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test the summarize_text function works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mchat_test_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Terapia cognitivo comportamental\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-60-8eff77954cd0>\u001b[0m in \u001b[0;36msummarize_text\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# Final summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Summarizing into overall summary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGPT_MODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     ) -> Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], bool, str]:\n\u001b[0;32m--> 288\u001b[0;31m         result = self.request_raw(\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m             \u001b[0m_thread_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_create_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             result = _thread_context.session.request(\n\u001b[0m\u001b[1;32m    597\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                 \u001b[0mabs_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    527\u001b[0m         }\n\u001b[1;32m    528\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    441\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    715\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    464\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    459\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1272\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Test the summarize_text function works\n",
        "chat_test_response = summarize_text(\"Terapia cognitivo comportamental\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c715f60d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c715f60d",
        "outputId": "79e2a390-4667-4152-ad94-744a3e8382f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- O artigo discute a necessidade de uma teoria de unificação para comportamentos cognitivos.\n",
            "- Propõe três ideias principais: a comunidade científica deve considerar seriamente o problema da unificação dos comportamentos cognitivos, a aplicação das regras de Newton para raciocinar sobre a natureza no estudo dos comportamentos cognitivos e o desenvolvimento de uma teoria científica que siga as regras de Newton e possa explicar todos os comportamentos cognitivos.\n",
            "- Destaca a importância da unificação nas teorias científicas e a natureza interdisciplinar do estudo dos comportamentos cognitivos.\n",
            "- Discute a história e o desenvolvimento da Inteligência Artificial (IA) e sua busca por alcançar a inteligência humana em computadores.\n",
            "- Menciona a necessidade de unificar diferentes teorias de comportamento humano e propõe o uso de computadores para testar o sucesso da unificação.\n",
            "- Destaca a importância da unificação da neurobiologia e psiquiatria no campo das ciências cognitivas.\n",
            "- Apresenta a Teoria de General Exocomportment (TGE) como uma possível solução para a unificação dos comportamentos cognitivos.\n",
            "- Discute a relação entre a computação e a TGE, enfatizando que a computação é uma noção fundamental do universo.\n",
            "- Apresenta a Teoria das Condições de Verdade Cognitiva (TCCV) como uma forma de descrever formalmente os exocomportamentos sensíveis.\n",
            "- Discute a importância da estruturação do conhecimento sobre a natureza em termos absolutos, em vez de relativos à percepção humana.\n",
            "- Destaca a necessidade de testar a TGE por meio de falsificabilidade e reprodutibilidade.\n",
            "- Apresenta exemplos de sistemas biológicos e não biológicos que podem ser usados para testar a TGE.\n",
            "- Discute a importância de experimentos de computador para testar a TGE e menciona a necessidade de realizar experimentos para validar os postulados da TGE.\n",
            "- Apresenta diferentes tipos de funções de previsão e suas arquiteturas.\n",
            "- Discute a viabilidade de conduzir experimentos para testar o segundo postulado da TGE usando simulações de computador.\n",
            "- Aborda a relação entre a TGE e a percepção, bem como a compatibilidade da TGE com a teoria da evolução.\n",
            "- Discute a relação entre a TGE e os comportamentos distribuídos.\n",
            "- Apresenta questões de pesquisa básica e aplicada relacionadas à TGE.\n",
            "- Conclui que a TGE pode abrir interessantes possibilidades no estudo do exocomportamento se for apoiada por experimentos.\n"
          ]
        }
      ],
      "source": [
        "print(chat_test_response[\"choices\"][0][\"message\"][\"content\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dab07e98",
      "metadata": {
        "id": "dab07e98"
      },
      "source": [
        "## Configure Agent\n",
        "\n",
        "We'll create our agent in this step, including a ```Conversation``` class to support multiple turns with the API, and some Python functions to enable interaction between the ```ChatCompletion``` API and our knowledge base functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "77a6fb4f",
      "metadata": {
        "id": "77a6fb4f"
      },
      "outputs": [],
      "source": [
        "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
        "def chat_completion_request(messages, functions=None, model=GPT_MODEL):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": \"Bearer \" + openai.api_key,\n",
        "    }\n",
        "    json_data = {\"model\": model, \"messages\": messages}\n",
        "    if functions is not None:\n",
        "        json_data.update({\"functions\": functions})\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            \"https://api.openai.com/v1/chat/completions\",\n",
        "            headers=headers,\n",
        "            json=json_data,\n",
        "        )\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(\"Unable to generate ChatCompletion response\")\n",
        "        print(f\"Exception: {e}\")\n",
        "        return e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "73f7672d",
      "metadata": {
        "id": "73f7672d"
      },
      "outputs": [],
      "source": [
        "class Conversation:\n",
        "    def __init__(self):\n",
        "        self.conversation_history = []\n",
        "\n",
        "    def add_message(self, role, content):\n",
        "        message = {\"role\": role, \"content\": content}\n",
        "        self.conversation_history.append(message)\n",
        "\n",
        "    def display_conversation(self, detailed=False):\n",
        "        role_to_color = {\n",
        "            \"system\": \"red\",\n",
        "            \"user\": \"green\",\n",
        "            \"assistant\": \"blue\",\n",
        "            \"function\": \"magenta\",\n",
        "        }\n",
        "        for message in self.conversation_history:\n",
        "            print(\n",
        "                colored(\n",
        "                    f\"{message['role']}: {message['content']}\\n\\n\",\n",
        "                    role_to_color[message[\"role\"]],\n",
        "                )\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "978b7877",
      "metadata": {
        "id": "978b7877"
      },
      "outputs": [],
      "source": [
        "# Initiate our get_articles and read_article_and_summarize functions\n",
        "arxiv_functions = [\n",
        "    {\n",
        "        \"name\": \"get_articles\",\n",
        "        \"description\": \"\"\"Use this function to get academic papers from arXiv to answer user questions.\"\"\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"query\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": f\"\"\"\n",
        "                            User query in JSON. Responses should be summarized and should include the article URL reference\n",
        "                            \"\"\",\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"query\"],\n",
        "        },\n",
        "        \"name\": \"read_article_and_summarize\",\n",
        "        \"description\": \"\"\"Use this function to read whole papers and provide a summary for users.\n",
        "        You should NEVER call this function before get_articles has been called in the conversation.\"\"\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"query\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": f\"\"\"\n",
        "                            Description of the article in plain text based on the user's query\n",
        "                            \"\"\",\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"query\"],\n",
        "        },\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0c88ae15",
      "metadata": {
        "id": "0c88ae15"
      },
      "outputs": [],
      "source": [
        "def chat_completion_with_function_execution(messages, functions=[None]):\n",
        "    \"\"\"This function makes a ChatCompletion API call with the option of adding functions\"\"\"\n",
        "    response = chat_completion_request(messages, functions)\n",
        "    full_message = response.json()[\"choices\"][0]\n",
        "    if full_message[\"finish_reason\"] == \"function_call\":\n",
        "        print(f\"Function generation requested, calling function\")\n",
        "        return call_arxiv_function(messages, full_message)\n",
        "    else:\n",
        "        print(f\"Function not required, responding to user\")\n",
        "        return response.json()\n",
        "\n",
        "\n",
        "def call_arxiv_function(messages, full_message):\n",
        "    \"\"\"Function calling function which executes function calls when the model believes it is necessary.\n",
        "    Currently extended by adding clauses to this if statement.\"\"\"\n",
        "\n",
        "    if full_message[\"message\"][\"function_call\"][\"name\"] == \"get_articles\":\n",
        "        try:\n",
        "            parsed_output = json.loads(\n",
        "                full_message[\"message\"][\"function_call\"][\"arguments\"]\n",
        "            )\n",
        "            print(\"Getting search results\")\n",
        "            results = get_articles(parsed_output[\"query\"])\n",
        "        except Exception as e:\n",
        "            print(parsed_output)\n",
        "            print(f\"Function execution failed\")\n",
        "            print(f\"Error message: {e}\")\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"function\",\n",
        "                \"name\": full_message[\"message\"][\"function_call\"][\"name\"],\n",
        "                \"content\": str(results),\n",
        "            }\n",
        "        )\n",
        "        try:\n",
        "            print(\"Got search results, summarizing content\")\n",
        "            response = chat_completion_request(messages)\n",
        "            return response.json()\n",
        "        except Exception as e:\n",
        "            print(type(e))\n",
        "            raise Exception(\"Function chat request failed\")\n",
        "\n",
        "    elif (\n",
        "        full_message[\"message\"][\"function_call\"][\"name\"] == \"read_article_and_summarize\"\n",
        "    ):\n",
        "        parsed_output = json.loads(\n",
        "            full_message[\"message\"][\"function_call\"][\"arguments\"]\n",
        "        )\n",
        "        print(\"Finding and reading paper\")\n",
        "        summary = summarize_text(parsed_output[\"query\"])\n",
        "        return summary\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"Function does not exist and cannot be called\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd3e7868",
      "metadata": {
        "id": "dd3e7868"
      },
      "source": [
        "## arXiv conversation\n",
        "\n",
        "Let's put this all together by testing our functions out in conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c39a1d80",
      "metadata": {
        "id": "c39a1d80"
      },
      "outputs": [],
      "source": [
        "# Start with a system message\n",
        "paper_system_message = \"\"\"You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\n",
        "You summarize the papers clearly so the customer can decide which to read to answer their question.\n",
        "You always provide the article_url and title so the user can understand the name of the paper and click through to access it.\n",
        "Begin!\"\"\"\n",
        "paper_conversation = Conversation()\n",
        "paper_conversation.add_message(\"system\", paper_system_message)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "253fd0f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "253fd0f7",
        "outputId": "10760cc1-1881-4cd1-db7a-7bd3dbb2a9d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function not required, responding to user\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Information theory is a branch of mathematics that deals with the quantification, storage, and communication of information. It provides a framework to understand the fundamental limits of communication, compression, and encryption. The relation between information theory and reality is that information theory provides a mathematical model to describe and analyze how information is processed and transmitted in various systems, including both human-made systems and natural systems found in the physical world. It has applications in various fields such as telecommunications, cryptography, data compression, machine learning, and more. By studying information theory, we can gain insights into the principles governing the transfer and transformation of information in the real world."
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Add a user message\n",
        "paper_conversation.add_message(\"user\", \"Hi, what is the relation between information theory and reality?\")\n",
        "chat_response = chat_completion_with_function_execution(\n",
        "    paper_conversation.conversation_history, functions=arxiv_functions\n",
        ")\n",
        "assistant_message = chat_response[\"choices\"][0][\"message\"][\"content\"]\n",
        "paper_conversation.add_message(\"assistant\", assistant_message)\n",
        "display(Markdown(assistant_message))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "3ca3e18a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "id": "3ca3e18a",
        "outputId": "599395b9-5e96-49ce-d270-61f83d224e51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function generation requested, calling function\n",
            "Finding and reading paper\n",
            "No papers searched yet, downloading first.\n",
            "A query enviada para arxiv é:  classical mechanical newton\n",
            "Failed to download Mechanics and Newton-Cartan-Like Gravity on the Newton-Hooke Space-time. Error: [Errno 2] No such file or directory: './data/papers/hep-th/0411004v2.Mechanics_and_Newton_Cartan_Like_Gravity_on_the_Newton_Hooke_Space_time.pdf'\n",
            "Failed to download Classical Field Theory and Analogy Between Newton's and Maxwell's Equations. Error: [Errno 2] No such file or directory: './data/papers/hep-th/9312009v1.Classical_Field_Theory_and_Analogy_Between_Newton_s_and_Maxwell_s_Equations.pdf'\n",
            "Papers downloaded, continuing\n",
            "Chunking text from paper\n",
            "Summarizing each chunk of text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:06<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarizing into overall summary\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Argumento chave:\n- O artigo discute o conceito de aleatoriedade na mecânica clássica e quântica.\n- O autor argumenta que o mundo determinístico clássico de Newton não se sustenta e que há uma aleatoriedade fundamental e irreversível na mecânica clássica.\n- O artigo sugere uma abordagem \"funcional\" para a mecânica clássica, onde a equação fundamental da dinâmica microscópica é a equação de Liouville para a função de distribuição de uma única partícula.\n- A equação de Newton aparece como uma equação aproximada que descreve a dinâmica dos valores médios de posição e momento.\n\nEvidências:\n- A trajetória newtoniana clássica não tem um significado físico direto, uma vez que as observações só podem ser feitas de números racionais, não de números reais arbitrários.\n- As soluções da equação de Liouville têm a propriedade de delocalização, o que explica a irreversibilidade.\n- A equação de movimento para uma única partícula em um campo potencial é dada por ∂ρ/∂t=−p/m∂ρ/∂q+∂V(q)/∂q∂ρ/∂p.\n- As funções de distribuição na mecânica clássica funcional e na mecânica quântica coincidem sob certas condições.\n\nPontos interessantes:\n- O artigo sugere interpretar a mecânica quântica de uma maneira que incorpore a aleatoriedade fundamental tanto na mecânica clássica quanto na quântica.\n- Em vez de um conjunto de eventos, o artigo sugere introduzir um conjunto de observadores.\n- O artigo também discute a incerteza na mecânica clássica e quântica e a introdução de variáveis contextuais na mecânica quântica.\n\nConclusão:\n- O artigo discute a abordagem funcional para a mecânica clássica, que se concentra em funções de distribuição em vez de trajetórias precisas de partículas.\n- O artigo sugere que tanto a mecânica clássica quanto a quântica contêm aleatoriedade fundamental, o que requer uma reavaliação da interpretação da mecânica quântica.\n- O artigo também menciona possíveis aplicações da mecânica funcional à mecânica estatística, teoria de campos, cosmologia e buracos negros."
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Add another user message to induce our system to use the second tool\n",
        "paper_conversation.add_message(\n",
        "    \"user\",\n",
        "    \"Can you read the classical mechanical newton paper for me and give me a summary\",\n",
        ")\n",
        "updated_response = chat_completion_with_function_execution(\n",
        "    paper_conversation.conversation_history, functions=arxiv_functions\n",
        ")\n",
        "display(Markdown(updated_response[\"choices\"][0][\"message\"][\"content\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add another user message to induce our system to use the second tool\n",
        "paper_conversation.add_message(\n",
        "    \"user\",\n",
        "    \"Explique mais profundamente o que significa o conceito da equialencia digital. quais os paradoxos criados pela velocidade máxima do universo. explique também como a simplicidade das leis matemáticas apoiam a ideia de que o mundo surge do processamento finito de informações. Explique quais as implicações caso essa teoria seja verdadeira.\",\n",
        ")\n",
        "updated_response = chat_completion_with_function_execution(\n",
        "    paper_conversation.conversation_history, functions=arxiv_functions\n",
        ")\n",
        "display(Markdown(updated_response[\"choices\"][0][\"message\"][\"content\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "vHVgFDT35WnB",
        "outputId": "5cf42b65-3674-41a9-99d3-ff982670a272"
      },
      "id": "vHVgFDT35WnB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function generation requested, calling function\n",
            "Finding and reading paper\n",
            "Chunking text from paper\n",
            "Summarizing each chunk of text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:11<00:00,  1.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarizing into overall summary\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Argumento chave:\n- O artigo explora a ideia de que o universo é uma realidade virtual criada por processamento de informações.\n- A criação do universo no Big Bang não seria paradoxal se fosse uma realidade virtual, pois todo sistema virtual precisa ser inicializado.\n- A ciência da informação moderna pode explicar propriedades físicas fundamentais como espaço, tempo, luz, matéria e movimento como derivadas do processamento de informações.\n\nEvidências:\n- Experimentos de física, como dilatação do tempo, curvatura do espaço, teleportação e criação do nada, estão impulsionando a necessidade de teorias estranhas na física.\n- As teorias atuais da física, como a mecânica quântica e a relatividade, têm sido bem-sucedidas em suas previsões, mas ainda carecem de uma fundamentação e não fazem sentido.\n\nPontos interessantes:\n- O autor argumenta que se a matéria, energia, carga, momento e spin são todas informações, então todas as leis de conservação poderiam se reduzir a uma lei de conservação da informação.\n- A simplicidade das leis matemáticas em descrever o mundo físico é vista como evidência de uma teoria de realidade virtual, já que cálculos frequentes em uma realidade virtual precisariam ser simples.\n- A incerteza complementar, descrita pelo princípio da incerteza de Heisenberg, é vista como uma propriedade da realidade na teoria de realidade virtual.\n\nConclusão:\n- O artigo conclui que a necessidade na física não é de mais provas ou aplicações, mas de mais compreensão.\n- A teoria de realidade virtual pode ajudar a explicar os mistérios da física moderna, como como as partículas parecem \"saber\" o que fazer e a velocidade máxima da luz.\n- A teoria de realidade virtual é uma opção lógica que deve ser considerada ao lado de outras teorias na física."
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}